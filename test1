import tensorflow as tf
from tensorflow.keras.layers import Layer, MultiHeadAttention, Embedding, Dense, LayerNormalization, Dropout
from tensorflow.keras.models import Model

class PositionalEncoding(Layer):
    def __init__(self, position, d_model):
        super(PositionalEncoding, self).__init__()
        self.pos_encoding = self.positional_encoding(position, d_model)

    def get_angles(self, position, i, d_model):
        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))
        return position * angles

    def positional_encoding(self, position, d_model):
        angle_rads = self.get_angles(
            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],
            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],
            d_model=d_model)
        # 将 sin 应用于数组中的偶数索引（indices）；2i
        sines = tf.math.sin(angle_rads[:, 0::2])
        # 将 cos 应用于数组中的奇数索引；2i+1
        cosines = tf.math.cos(angle_rads[:, 1::2])

        pos_encoding = tf.concat([sines, cosines], axis=-1)
        pos_encoding = pos_encoding[tf.newaxis, ...]
        return tf.cast(pos_encoding, tf.float32)

    def call(self, inputs):
        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]

class TransformerBlock(Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerBlock, self).__init__()
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential(
            [Dense(ff_dim, activation="relu"), Dense(embed_dim),]
        )
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

# 创建 Transformer 模型
def transformer_model(input_shape, embed_dim, num_heads, ff_dim, position, rate=0.1):
    inputs = tf.keras.Input(shape=input_shape)
    x = Embedding(input_dim=10000, output_dim=embed_dim)(inputs)
    x = PositionalEncoding(position, embed_dim)(x)
    x = TransformerBlock(embed_dim, num_heads, ff_dim, rate)(x)
    x = GlobalAveragePooling1D()(x)
    x = Dropout(0.1)(x)
    x = Dense(20, activation="relu")(x)
    x = Dropout(0.1)(x)
    outputs = Dense(10, activation="softmax")(x)
    return Model(inputs=inputs, outputs=outputs)

model = transformer_model((100,), 32, 2, 32, 100)
model.summary()




import tensorflow as tf
from tensorflow.keras.layers import Layer, MultiHeadAttention, Embedding, Dense, LayerNormalization, Dropout, Flatten, Reshape

class PositionalEncoding(Layer):
    # ... （与之前的代码相同）

class TransformerBlock(Layer):
    # ... （与之前的代码相同）

def ImageTransformer(input_shape, num_patches, embed_dim, num_heads, ff_dim, num_blocks, num_classes):
    inputs = tf.keras.Input(shape=input_shape)
    
    # 将图像分成小块并线性化
    h, w = input_shape[:2]
    patch_size = h // num_patches
    x = Reshape((num_patches, patch_size * patch_size * 3))(inputs)
    
    # 对每个块进行嵌入
    x = Dense(embed_dim)(x)
    
    # 添加位置编码
    x = PositionalEncoding(num_patches, embed_dim)(x)
    
    # 通过 Transformer 编码器
    for _ in range(num_blocks):
        x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)
    
    # 全局平均池化
    x = tf.reduce_mean(x, axis=1)
    
    # 分类头
    outputs = Dense(num_classes, activation='softmax')(x)
    
    return tf.keras.Model(inputs=inputs, outputs=outputs)

model = ImageTransformer((32, 32, 3), 8, 64, 2, 128, 2, 10)
model.summary()
